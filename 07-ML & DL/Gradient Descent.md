# Gradient Descent
## π’΅ Gradient Descentλ€?
<p align="center"><img src="./images/gd.jpg" align="center" width="50%"></p>

`κ²½μ‚¬ ν•κ°•λ²•(Gradient Descent)`μ€ λΉ„μ© ν•¨μλ¥Ό μµμ†ν™”ν•κΈ° μ„ν•΄ νλΌλ―Έν„°($\theta$)μ— λ€ν• λΉ„μ©ν•¨μμ ν„μ¬ κ·Έλ λ””μ–ΈνΈκ°€ κ°μ†ν•λ” λ°©ν–¥μΌλ΅ νλΌλ―Έν„°λ¥Ό μ΅°μ •ν•λ” λ°©λ²•μ…λ‹λ‹¤.
$$\theta^{\text (next\ step)} = \theta-\eta\nabla_\theta LOSS(\theta)$$
- $\theta$ : μ΅°μ •ν•  νλΌλ―Έν„°
- $\eta$ : ν•™μµλ¥ 
- $\nabla_\theta LOSS(\theta)$ : $\theta$μ— λ€ν• λΉ„μ©ν•¨μμ κ·Έλ λ””μ–ΈνΈ λ²΅ν„°

<λ™μ‘ λ°©μ‹>
1. μ΄κΈ°ν™”: λ¨Όμ € μµμ ν™”ν•λ ¤λ” ν•¨μμ μ‹μ‘μ μ„ μ΄κΈ°ν™”ν•©λ‹λ‹¤. μ΄ μ΄κΈ°ν™” λ‹¨κ³„μ—μ„λ” λ³΄ν†µ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ λ¬΄μ‘μ„λ΅ μ„¤μ •ν•κ±°λ‚ μ‘μ€ κ°’μΌλ΅ μ΄κΈ°ν™”ν•©λ‹λ‹¤.
2. κΈ°μΈκΈ° κ³„μ‚°: μ£Όμ–΄μ§„ μ„μΉμ—μ„ ν•¨μμ κΈ°μΈκΈ°(gradient)λ¥Ό κ³„μ‚°ν•©λ‹λ‹¤. κΈ°μΈκΈ°λ” ν•΄λ‹Ή μ„μΉμ—μ„ ν•¨μ κ°’μ΄ κ°€μ¥ λΉ λ¥΄κ² μ¦κ°€ λλ” κ°μ†ν•λ” λ°©ν–¥κ³Ό ν¬κΈ°λ¥Ό λ‚νƒ€λƒ…λ‹λ‹¤.
3. μ—…λ°μ΄νΈ: κ³„μ‚°λ κΈ°μΈκΈ°λ¥Ό μ΄μ©ν•μ—¬ ν„μ¬ μ„μΉμ—μ„ μ΅°κΈ λ” μµμ†κ°’ μ½μΌλ΅ μ΄λ™ν•©λ‹λ‹¤. μ΄λ™ κ±°λ¦¬λ” μ‚¬μ©μκ°€ μ„¤μ •ν• ν•™μµλ¥ (learning rate)μ— μν•΄ κ²°μ •λ©λ‹λ‹¤. 
4. λ°λ³µ: μ„ κ³Όμ •μ„ μ—¬λ¬ λ² λ°λ³µν•μ—¬ μµμ ν™”ν•λ ¤λ” ν•¨μμ μµμ†κ°’μ— μ μ°¨ μλ ΄ν•©λ‹λ‹¤. κΈ°μΈκΈ°λ¥Ό κ³„μ‚°ν•κ³  μ—…λ°μ΄νΈν•λ” κ³Όμ •μ„ λ°λ³µν•λ©΄μ„ μµμ μ νλΌλ―Έν„°(κ°€μ¤‘μΉμ™€ νΈν–¥)λ¥Ό μ°Ύμµλ‹λ‹¤.


<br>

## π“‘ κΌ¬λ¦¬μ§λ¬Έ
### Local Minima λ¬Έμ μ—λ„, Gradient Descentκ°€ μ‚¬μ©λ  μ μλ” μ΄μ λ” λ¬΄μ—‡μΈκ°€μ”?
- [λ…Όλ¬Έ](https://proceedings.neurips.cc/paper_files/paper/2014/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract.html)μ— λ”°λ¥΄λ©΄, κ³ μ°¨μ›μ—μ„ weightμ λ¨λ“  μ”μ†κ°€ local minimaμ— λΉ μ§€λ” κ²½μ°λ” κ±°μ λ°μƒν•μ§€ μ•κΈ° λ•λ¬Έμ— local minima λ¬Έμ λ” μ‚¬μ‹¤μƒ κ³ μ°¨μ› λ°μƒν•κΈ° λ§¤μ° ν¬κ·€ν• κ²½μ°λΌκ³  μ£Όμ¥ν•©λ‹λ‹¤. λν•, local minimaκ°€ λ°μƒν•λ‹¤ ν•΄λ„ μ΄λ” μ‚¬μ‹¤ global minimumμ΄κ±°λ‚ global minimumκ³Ό μ μ‚¬ν• κ°’μ„ κ°€μ§€κ² λμ–΄ λ¬Έμ κ°€ μ—†λ” κ²½μ°κ°€ λ§μµλ‹λ‹¤.
- Momentum, RMSProp, Adamλ“±μ κ°μ„ λ Optimizerλ” Local Minimaλ¥Ό ν”Όν•κΈ° μ„ν• μλ‹¨μ„ μ κ³µν•©λ‹λ‹¤.
    <p align="center"><img src="./images/optimizer.jpg" align="center" width="100%"></p>

### λΉ„μ©ν•¨μ MSEμ— λ€ν• κ²½μ‚¬ν•κ°•λ²•μ„ κµ¬ν„ν•΄λ³΄μ„Έμ”
1. λΉ„μ©ν•¨μμ νΈλ„ν•¨μ κµ¬ν•κΈ°

$$\frac {\partial} {\partial \theta_j}MSE(\theta) = \frac {2} {m}\sum^m_{i=1}\left(\theta^T {\bf x}^{(i)}-y^{(i)}\right)x^{(i)}_j$$

2. λΉ„μ© ν•¨μμ κ·Έλ λ””μ–ΈνΈ λ²΅ν„° κµ¬ν•κΈ°

$$\nabla_\theta MSE(\theta) = \begin{bmatrix} 
{\frac {\partial} {\partial\theta_0}MSE(\theta)} \\
{\frac {\partial} {\partial\theta_1}MSE(\theta)} \\
\vdots \\
{\frac {\partial} {\partial\theta_n}MSE(\theta)}
\end{bmatrix} = \frac {2} {m} {\bf X}^T({\bf X\theta-y})$$

3. κ²½μ‚¬ν•κ°•λ²• κµ¬ν„ν•κΈ°

$$\theta^{\text (next\ step)} = \theta-\eta\nabla_\theta LOSS(\theta)$$

```python
import numpy as np
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.rand(100, 1)

X_b = np.c_[np.ones((100, 1)), X] # λ¨λ“  μƒν”μ— x0 = 1(bias)μ„ μ¶”κ°€

eta = 0.1
epochs = 1000
m = 100

theta = np.random.randn(2,1) # λ¬΄μ‘μ„ μ΄κΈ°ν™”

for epoch in range(epochs):
	gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) # MSEμ κ·Έλ λ””μ–ΈνΈ κµ¬ν•κΈ°
	theta = theta - eta * gradients # νλΌλ―Έν„° μ—…λ°μ΄νΈ

'''
>>> theta
array([[4.21509616],
			 [2.77011339]])
'''
```
### Batch Gradient Descent, Stochastic Gradient Descent, Mini Batch Gradient Descentλ¥Ό λΉ„κµν•΄λ³΄μ„Έμ”.
**Batch Gradient Descent(λ°°μΉ κ²½μ‚¬ ν•κ°•λ²•)**

- λ§¤ μ¤ν… μ „μ²΄ ν›λ ¨ μ„ΈνΈμ—μ„ κ²½μ‚¬ ν•κ°•λ²•μ„ μ§„ν–‰ν•λ” λ°©λ²•μΌλ΅, μ„ μ•κ³ λ¦¬μ¦κ³Ό κ°™μµλ‹λ‹¤.
- λ”°λΌμ„ ν›λ ¨ μ„ΈνΈκ°€ μ»¤μ§€λ©΄ λ§¤μ° λλ ¤μ§€κ² λλ‹¤λ” λ‹¨μ μ΄ μμµλ‹λ‹¤.

**Stochastic Gradient Descent(ν™•λ¥ μ  κ²½μ‚¬ ν•κ°•λ²•)**
<p align="center"><img src="./images/sgd.jpg" align="center" width="100%"></p>

- λ§¤ μ¤ν… ν• κ°μ μƒν”μ„ λ¬΄μ‘μ„λ΅ μ„ νƒν•κ³ , κ·Έ ν•λ‚μ μƒν”μ— λ€ν• κ·Έλ λ””μ–ΈνΈλ¥Ό κ³„μ‚°ν•΄ κ²½μ‚¬ ν•κ°•λ²•μ„ μ§„ν–‰ν•λ” λ°©λ²•μ…λ‹λ‹¤.
- λ§¤ μ¤ν…μ—μ„ λ‹¤λ¤„μ•Ό ν•  λ°μ΄ν„°κ°€ λ§¤μ° μ κΈ° λ•λ¬Έμ— μ•κ³ λ¦¬μ¦μ΄ ν›¨μ”¬ λΉ λ¥΄λ©°, λ§¤ μ¤ν…λ§λ‹¤ λ©”λ¨λ¦¬μ— ν•΄λ‹Ή μƒν”λ§ μ μ¬ν•λ©΄ λλ―€λ΅, ν›¨μ”¬ ν° ν›λ ¨ μ„ΈνΈλ„ ν›λ ¨μ‹ν‚¬ μ μμµλ‹λ‹¤.
- ν•μ§€λ§ λ°°μΉ κ²½μ‚¬ ν•κ°•λ²•μ— λΉ„ν•΄ ν›¨μ”¬ λ¶μ•μ •ν•©λ‹λ‹¤. lossκ°€ μµμ†κ°’μ— μλ ΄ν•  λ•κΉμ§€ λ¶€λ“λ½κ² κ°μ†ν•μ§€ μ•κ³  μ„μ•„λλ΅ μ”λ™μΉλ©° ν‰κ· μ μΌλ΅ κ°μ†ν•©λ‹λ‹¤.

**Mini-Batch Gradient Descent(λ―Έλ‹λ°°μΉ κ²½μ‚¬ ν•κ°•λ²•)**
<p align="center"><img src="./images/minibatch.jpg" align="center" width="80%"></p>

- λ§¤ μ¤νƒ­ **λ―Έλ‹λ°°μΉ**λΌ λ¶€λ¥΄λ” μ„μμ μ‘μ€ μƒν” μ„ΈνΈμ— λ€ν•΄ κ·Έλ λ””μ–ΈνΈλ¥Ό κ³„μ‚°ν•΄ κ²½μ‚¬ ν•κ°•λ²•μ„ μ§„ν–‰ν•λ” λ°©λ²•μ…λ‹λ‹¤.
- ν™•λ¥ μ  κ²½μ‚¬ ν•κ°•λ²•μ— λΉ„ν•΄ GPUμ λ³‘λ ¬ μ²λ¦¬ μ—°μ‚°μΌλ΅ μ–»λ” μ„±λ¥ ν–¥μƒμ΄ μμ„ μ μμµλ‹λ‹¤.
- λ°°μΉ κ²½μ‚¬ ν•κ°•λ²•λ³΄λ‹¤ λΉ λ¥΄κ³ , SGDλ³΄λ‹¤ λ‚®μ€ μ¤μ°¨μ¨μ„ κ°€μ§€κΈ° λ•λ¬Έμ λ‘ λ°©λ²•μ„ μ μ¶©ν• λ°©λ²•μΌλ΅ μƒκ°ν•  μ μμµλ‹λ‹¤.
<br>

## π κΌ¬κΌ¬λ¬΄
### μ†μ‹¤ ν•¨μλ” λ¬΄μ—‡μΈκ°€μ”?
μ§€λ„ν•™μµ(Supervised Learning) μ‹ μ•κ³ λ¦¬μ¦μ΄ μμΈ΅ν• κ°’κ³Ό μ‹¤μ  μ •λ‹µμ μ°¨μ΄λ¥Ό λΉ„κµν•κΈ° μ„ν• ν•¨μμ…λ‹λ‹¤. μ¦‰, 'ν•™μµ μ¤‘μ— μ•κ³ λ¦¬μ¦μ΄ μ–Όλ§λ‚ μλ» μμΈ΅ν•λ” μ •λ„'λ¥Ό ν™•μΈν•κΈ° μ„ν• ν•¨μλ΅μ¨ μµμ ν™”(Optimization)λ¥Ό μ„ν•΄ μµμ†ν™”ν•λ” κ²ƒμ΄ λ©μ μΈ ν•¨μμ…λ‹λ‹¤. κ·Έλμ„ `μ†μ‹¤ ν•¨μ`λ¥Ό `λ©μ  ν•¨μ(Objective Function)`λΌκ³ λ„ λ¶€λ¦…λ‹λ‹¤. μ΄μ™Έμ—λ„ μ†μ‹¤ ν•¨μλ” λ¶„μ•Όμ— λ”°λΌ `λΉ„μ© ν•¨μ(Cost Function)`, `μ—λ„μ§€ ν•¨μ(Energy Function)` λ“±μΌλ΅ λ‹¤μ–‘ν•κ² λ¶€λ¥΄κΈ°λ„ ν•©λ‹λ‹¤. μ†μ‹¤ ν•¨μλ¥Ό ν†µν•΄ λ¨λΈ ν•™μµ μ¤‘μ— μ†μ‹¤(loss)μ΄ μ»¤μ§μλ΅ ν•™μµμ΄ μ μ• λκ³  μλ‹¤κ³  ν•΄μ„ν•  μ μκ³ , λ°λ€λ΅ μ†μ‹¤μ΄ μ‘μ•„μ§μλ΅ ν•™μµμ΄ μ μ΄λ£¨μ–΄μ§€κ³  μλ‹¤κ³  ν•΄μ„ν•©λ‹λ‹¤.

### Learning Rate(LR)μ΄ μ™ μ¤‘μ”ν•κ°€μ”?
<p align="center"><img src="./images/localminima.jpg" align="center" width="70%"></p>

- νλΌλ―Έν„° κ°’μ„ μ—…λ°μ΄νΈ ν•λ” Step Sizeλ¥Ό LRλΌλ” ν•μ΄νΌ νλΌλ―Έν„°λ΅ λ§¤μ° μ¤‘μ”ν•κ² μ΅°μ • λμ–΄μ•Ό ν•©λ‹λ‹¤.
- λ„λ¬΄ ν° κ°’μ„ μ£Όκ² λμ—μ„ λ•λ” μλ ΄ μ†λ„κ°€ λΉ¨λΌμ§ μ μμ§€λ§ μµμ†κ°’μ„ κ³„μ‚°ν•λ„λ΅ μλ ΄ν•μ§€ λ»ν•κ³  minina μ§€μ μ„ μ¤ν‚µ ν•  μ μκ² λ©λ‹λ‹¤.
- λ„λ¬΄ μ‘μ€ κ°’μ„ μ£Όκ² λλ‹¤λ©΄ ν•™μµ μ†λ„κ°€ κµ‰μ¥ν λλ ¤μ§€λ©° local minimaμ— λΉ μ Έ ν•™μµμ΄ μ›ν™ν•κ² μ§„ν–‰λμ§€ λ»ν•κ² λ©λ‹λ‹¤.
<br>

## π“ Reference
- [ν•Έμ¦μ¨ λ¨Έμ‹ λ¬λ‹ 2ν - 4μ¥ λ¨λΈ ν›λ ¨]
- [ν‹°μ¤ν† λ¦¬ - Gradient Descent νƒμƒ‰ λ°©λ²•](https://darkpgmr.tistory.com/133) 
- [ν‹°μ¤ν† λ¦¬ - [Deep Learning] ν™•λ¥ μ  κ²½μ‚¬ ν•κ°•λ²•(Stochastic Gradient Descent)](https://go-hard.tistory.com/11)
